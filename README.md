# AdvancedAI_Project1
Approximate Inference in Bayesian Networks 

In this project, I implemented 3 approximate inference algorithms: 
  - Likelihood weighting
  - Gibbs sampling
  - Metropolis-Hastings

I compared all results to the exact inference algorithm given to us. 


The instructions for the project, rather than being given in a pdf, are supplied below: 

----------------------------------------------------------------------------------------------------------------------

Implement approximate (likelihood weighting, Gibbs sampling, Metropolis Hastings (MH)) methods for Bayesian networks (BNs).

Note on Metropolis Hastings implementation:  There are a few plausible ways to interpet the MH sampling method as described in page 447 of the textbook. 
I suggest the most straightforward implementation: Given current sample, x, perform Gibbs sampling with probability p, else generate a random sample, x', with the WEIGHTED-SAMPLE algorithm.  Samples generated by both the processes are always accepted.  
A possible alternate interpretation of the previous approach is the following.  Always accept the next state generated by the Gibbs sampler.  For x' generated with the WEIGHTED-SAMPLE algorithm, let the weight returned be π(x').  Also let π(x) be the weight of the current sample.  Then, accept x' as the new state if π(x')>π(x), i.e., accept if the new state is more likely. 

In either case, you should report comparative results from experiments run with the following values for p: 0.75, 0.85, and 0.95.
Use the provided BN generator to create networks of different sizes, some of which are polytrees and others are not.
Investigate and report relative scale-up properties, by varying size of the BN, of different inference algorithms.  Your report should provide details about
how you chose the data set to evaluate the scale-up properties, report run times including mean and standard deviations (use candlestick plots),
variation in converged probabilities (difference from the exact inference result on polytrees),
 relative effectiveness of working with downstream vs upstream evidence compared to query variables,
relative effectiveness when CPT entries are close to/distant from 0/1 values.

Note: Code for variable elimination algorithm will be shared with you.  You will need to include in your report a comparison of the accuracy of different approximation algorithms that you are implementing when compared with the variable elimination algorithm (exact inference).  For this set of comparison experiments, use only polytree BNs. 
Code provided:
Python code, developed by Chad Crawford, for generating BNs.
Python code, developed by Robert Geraghty, for the enumeration-ask algorithm for exact inference in BNs.
